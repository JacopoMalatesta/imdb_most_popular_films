{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- capire come scrapare meglio user review counts\n",
    "- join df, df2, (df3)\n",
    "- pulizia dati (contare nulle, casting)\n",
    "- script con funzioni???\n",
    "- cartella scripts e data\n",
    "- aggiungere table of contents\n",
    "- commentare tutto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGNNMM5OucPo"
   },
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Tq5zlLcDuCkM"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GySk7pbunwY"
   },
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-6kyKK3UprX"
   },
   "source": [
    "IMDB doesn't have an official API. However, there are two unofficial APIs providing access to IMDB data: The Open Movie Database (OMDb) and The Movie Database (TMDB). In this project we'll rely on the latter as it doesn't enforce a rate limit. We're also going to supplement the API data with scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt0ZSUT3Xm3o"
   },
   "source": [
    "The TMDB API works by supplying the ID of the film(s) whose info we'd like to retrieve. The IDs consist of a double \"t\" followed by seven or eight digits. They can be found inside a film URL. For example, \"tt0050782\" is the ID for this film: https://www.imdb.com/title/tt0050782/. \n",
    "\n",
    "The first step will thus be retrieving the IDs of the 10 000 most popular films on IMDB, which can be found in this [playlist](https://www.imdb.com/search/keyword/?mode=detail&page=1&title_type=movie&ref_=kw_ref_rt_vt&num_votes=5000%2C&sort=num_votes,desc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruIEsF-V4A7N"
   },
   "source": [
    "## Scraping the film IDs and the URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MN3DXnxMYrbc"
   },
   "source": [
    "To scrape the film IDs of our 10 000 films we'll need to iterate a for loop over the first 200 pages in the playlist (each containing 50 films). The film ID is the value of the 'href' attribute inside h3 elements with the \"lister-item-header\" class. We'll write a simple regex to extract the film ID. We'll also scrape the entire URL: it's going to come in handy later when we do the scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N0QXTJHZwTUc"
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.imdb.com', port=443): Max retries exceeded with url: /search/keyword/?mode=detail&page=91&title_type=movie&ref_=kw_ref_rt_vt&num_votes=5000%2C&sort=num_votes,desc (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000193B9BA87F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    160\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             raise NewConnectionError(\n\u001b[0m\u001b[0;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x00000193B9BA87F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    727\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.imdb.com', port=443): Max retries exceeded with url: /search/keyword/?mode=detail&page=91&title_type=movie&ref_=kw_ref_rt_vt&num_votes=5000%2C&sort=num_votes,desc (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000193B9BA87F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cdeea0eea30f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m201\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m   \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"https://www.imdb.com/search/keyword/?mode=detail&page={i}&title_type=movie&ref_=kw_ref_rt_vt&num_votes=5000%2C&sort=num_votes,desc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.imdb.com', port=443): Max retries exceeded with url: /search/keyword/?mode=detail&page=91&title_type=movie&ref_=kw_ref_rt_vt&num_votes=5000%2C&sort=num_votes,desc (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000193B9BA87F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "film_ids = []\n",
    "film_urls = []\n",
    "\n",
    "for i in range(1, 201):\n",
    "  \n",
    "  content = requests.get(f\"https://www.imdb.com/search/keyword/?mode=detail&page={i}&title_type=movie&ref_=kw_ref_rt_vt&num_votes=5000%2C&sort=num_votes,desc\").content\n",
    "  \n",
    "  soup = BeautifulSoup(content)\n",
    "\n",
    "  for film in soup.find_all(\"h3\", class_=\"lister-item-header\"):\n",
    "\n",
    "    link = film.find(\"a\").get(\"href\")\n",
    "    film_id = re.findall(pattern = r\"tt\\d+\", string = link)[0]\n",
    "    film_ids.append(film_id)\n",
    "\n",
    "    film_url = \"https://www.imdb.com\" + link\n",
    "    film_urls.append(film_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GwT4Wuv61we"
   },
   "source": [
    "## API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51KySz-jZsxT"
   },
   "source": [
    "We'll now write a function to request the data from TMDB. The function takes a vector of film IDs as input and it returns a dataframe. For each ID it sends a GET request to the API. If the status code is equal to 200 (i.e. the request has been successful) it appends the data to a Pandas dataframe. Otherwise, it just appends the film_id and fills the remaining columns with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pn5hoWsWdNZZ"
   },
   "outputs": [],
   "source": [
    "def build_film_df(film_ids):\n",
    "    \n",
    "  api_key = os.environ.get(\"tmdb_api_key\")\n",
    "\n",
    "  df = pd.DataFrame(columns = [\"id\", \"title\", \"release_date\", \"runtime\", \"country\", \"language\", \n",
    "                               \"genre\", \"studios\", \"budget\", \"revenue\"])\n",
    "\n",
    "  for film_id in film_ids:\n",
    "      \n",
    "      response = requests.get(f\"https://api.themoviedb.org/3/movie/{film_id}?api_key={api_key}\")\n",
    "\n",
    "      if response.status_code == 200:  \n",
    "\n",
    "        response_json = response.json()\n",
    "\n",
    "        df = df.append({\"id\": response_json[\"imdb_id\"],\n",
    "                      \"title\": response_json[\"title\"],\n",
    "                      \"release_date\": response_json[\"release_date\"],\n",
    "                      \"runtime\": response_json[\"runtime\"],\n",
    "                      \"country\": ';'.join([country['name'] for country in response_json[\"production_countries\"]]),\n",
    "                      \"language\": ';'.join([language[\"english_name\"] for language in response_json[\"spoken_languages\"]]),\n",
    "                      \"genre\": ';'.join([genre[\"name\"] for genre in response_json[\"genres\"]]),\n",
    "                      \"studios\": ';'.join([company[\"name\"] for company in response_json['production_companies']]),\n",
    "                      \"budget\": response_json['budget'],\n",
    "                      \"revenue\": response_json[\"revenue\"]}, \n",
    "                       ignore_index = True)\n",
    "        \n",
    "      else:\n",
    "        df = df.append({\"id\": film_id}, ignore_index = True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it on the film IDs we just scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "ckKclT-ouHfd",
    "outputId": "d438579e-4d83-4c41-aad8-e53f0c02135b"
   },
   "outputs": [],
   "source": [
    "df = build_film_df(film_ids)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZmSWuoY8zvF"
   },
   "source": [
    "Let's have a look at the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "9Z4dClmq8WuR",
    "outputId": "3efdddf4-1f69-4614-b5c4-2a02803f79f0"
   },
   "outputs": [],
   "source": [
    "df[df['title'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VroIT9ExpVtY"
   },
   "source": [
    "## Scraping additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOgUWYm7bRqV"
   },
   "source": [
    "TMDB API has some limits: it doesn't provide data about the people who worked on a film (directors, writer, actors etc.) and its rating data is inaccurate. We'll thus integrate the data we just got by scraping some more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdfF63LCTwRi"
   },
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KytHOgpT0HC"
   },
   "source": [
    "To implement sound software engineering principles we'll scrape the data by building a function for each type of data we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdS235LCUFDf"
   },
   "source": [
    "#### Film ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNSWeewsQ3Gt"
   },
   "outputs": [],
   "source": [
    "def scrape_film_id(soup):\n",
    "    \n",
    "    try:\n",
    "        film_id = soup.find(\"meta\", {\"property\": \"imdb:pageConst\"}).get(\"content\")\n",
    "    except:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return film_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WpHvI39UHtL"
   },
   "source": [
    "#### Directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwLDT6CW1tL0"
   },
   "outputs": [],
   "source": [
    "def scrape_director(soup):\n",
    "    \n",
    "    try:  \n",
    "        a_tags = soup.find_all(href = re.compile(\"tt_ov_dr\"), class_=\"ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link\")\n",
    "    \n",
    "        directors = list(set([a.text for a in a_tags]))\n",
    "    \n",
    "        directors = ';'.join(directors)\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "    else:\n",
    "        return directors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_writer(soup):\n",
    "    \n",
    "    try:\n",
    "        a_tags = soup.find_all(href = re.compile(\"tt_ov_wr\"), class_=\"ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link\")\n",
    "        \n",
    "        writers = list(set([a.text for a in a_tags]))\n",
    "        \n",
    "        writers = ';'.join(writers)\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "    else:\n",
    "        return writers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY0xRacoUKxG"
   },
   "source": [
    "#### IMDB average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHuWP7Pe8EHq"
   },
   "outputs": [],
   "source": [
    "def scrape_imdb_rating(soup):\n",
    "    \n",
    "    try: \n",
    "        pattern = r'(?<=\"ratingValue\":)[\\d.]+(?=},\"contentRating\")'\n",
    "        string = str(soup.find(\"script\", {\"type\": \"application/ld+json\"}))\n",
    "        rating = re.findall(pattern = pattern, string = string)[0]\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return rating\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63sBX-TwUN1q"
   },
   "source": [
    "#### IMDB rating count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7u68JHWr_1oe"
   },
   "outputs": [],
   "source": [
    "def scrape_rating_count(soup):\n",
    "    \n",
    "    try:\n",
    "        pattern = r'(?<=\"ratingCount\":)[\\d.]+'\n",
    "        string = str(soup.find(\"script\", {\"type\": \"application/ld+json\"}))\n",
    "        rating_count = re.findall(pattern = pattern, string = string)[0]\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return rating_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne8ybLy_UPrk"
   },
   "source": [
    "#### Metascore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzWpD51AAhe5"
   },
   "outputs": [],
   "source": [
    "def scrape_metascore(soup):\n",
    "    \n",
    "    try:\n",
    "        metascore = soup.find(\"span\", class_=\"score-meta\").text \n",
    "    \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return metascore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YwH147RUSXB"
   },
   "source": [
    "#### User review count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLb_JqIRnKls"
   },
   "outputs": [],
   "source": [
    "def scrape_user_review_count(soup):\n",
    "    \n",
    "    try: \n",
    "        pattern = r'(?<=\"total\":)\\d+(?=,\"__typename\":\"ReviewsConnection\"},\"criticReviewsTotal\":)'\n",
    "  \n",
    "        string = str(soup.find(\"script\", {'id': '__NEXT_DATA__'}))\n",
    "    \n",
    "        user_review_count = re.findall(pattern = pattern, string = string)[0]\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return user_review_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0WXIPIWUWHl"
   },
   "source": [
    "#### Critic review count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_critic_review_count(soup):\n",
    "    \n",
    "    try:   \n",
    "        spans = soup.find_all(\"span\", class_=\"three-Elements\") \n",
    "        \n",
    "        string = list(filter(lambda x: 'Critic' in str(x), spans))[0].text\n",
    "        \n",
    "        critic_review_count = re.findall(r'\\d+', string)[0]\n",
    "    \n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "    else:\n",
    "        return critic_review_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_color(soup):\n",
    "    \n",
    "    try:\n",
    "        pattern = r'(?<=\"text\":\")\\w+(?=\",\"attributes\":\\[\\],\"__typename\":\"Coloration\"})'\n",
    "        \n",
    "        string = str(soup.find(\"script\", {\"type\": \"application/json\"}))\n",
    "    \n",
    "        color = re.findall(pattern = pattern, string = string)[0]\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_aspect_ratio(soup):\n",
    "    try:\n",
    "        pattern = r'(?<=\"aspectRatio\":\")[\\d.\\s:]+'\n",
    "    \n",
    "        string = str(soup.find(\"script\", {\"type\": \"application/json\"}))\n",
    "    \n",
    "        aspect_ratio = re.findall(pattern = pattern, string = string)[0] \n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "    else:\n",
    "        return aspect_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRaqDP82V4Xh"
   },
   "source": [
    "### Creating the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we send 10 000 requests at once we're likely going to run into problems with IMDB, so let's split the film_urls list into 5 equally-sized chunks and let's scrape each chunk at a time by waiting five minutes before moving to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PuxtrS5V8IQ"
   },
   "outputs": [],
   "source": [
    "def build_scraped_df(urls):\n",
    "\n",
    "  df = pd.DataFrame(columns = [\"id\", \"director\", \"writer\", \"imdb_rating\", \"imdb_rating_count\", \"metascore\", \n",
    "                               \"user_review_count\", \"critic_review_count\", \"color\", \"aspect_ratio\"])\n",
    "\n",
    "  chunks = np.array_split(urls, 5)\n",
    "\n",
    "  for chunk in chunks:\n",
    "    for url in chunk:\n",
    "      content = requests.get(url).content\n",
    "      soup = BeautifulSoup(content)\n",
    "\n",
    "      df = df.append({\"id\": scrape_film_id(soup),\n",
    "                      \"director\": scrape_director(soup),\n",
    "                      \"writer\": scrape_writer(soup),\n",
    "                      \"imdb_rating\": scrape_imdb_rating(soup),\n",
    "                      \"imdb_rating_count\": scrape_rating_count(soup),\n",
    "                      \"metascore\": scrape_metascore(soup),\n",
    "                      \"user_review_count\": scrape_user_review_count(soup),\n",
    "                      \"critic_review_count\": scrape_critic_review_count(soup),\n",
    "                      \"color\": scrape_color(soup),\n",
    "                      \"aspect_ratio\": scrape_aspect_ratio(soup)}, \n",
    "                      ignore_index = True)\n",
    "\n",
    "    time.sleep(60*5)\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "Sqmef_HbZ6fi",
    "outputId": "a5e98cbb-4ae7-4444-9e39-bfe70225c3d4"
   },
   "outputs": [],
   "source": [
    "df2 = build_scraped_df(film_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "Sqmef_HbZ6fi",
    "outputId": "a5e98cbb-4ae7-4444-9e39-bfe70225c3d4"
   },
   "outputs": [],
   "source": [
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the null values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is each id unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping people data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, TMDB API doesn't provide much data about the people who work on a film, so we'll need to scrape that data ourselves. These pieces of info aren't located in the regular film pages but in the \"Full cast and crew\" pages. So before scraping the data we first need to collect all these pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_pages = []\n",
    "\n",
    "for url in film_urls:\n",
    "        \n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    a_target = soup.find(href = re.compile(\"tt_cl_sm\"), class_=\"ipc-metadata-list-item__label ipc-metadata-list-item__label--link\")\n",
    "    \n",
    "    crew_page = \"https://www.imdb.com\" + a_target.get(\"href\")\n",
    "    \n",
    "    crew_pages.append(crew_page)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQtgCMgbUgCS"
   },
   "source": [
    "#### Full cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the artists can be found inside the table with the \"cast-list\" class: they're the alternative text of the images inside the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_actors(soup):\n",
    "    \n",
    "    try:   \n",
    "        images = soup.find(\"table\", class_=\"cast_list\").find_all(\"img\") \n",
    "        \n",
    "        actors = [img.get(\"alt\") for img in images]  \n",
    "        \n",
    "        actors = ';'.join(actors)\n",
    "    \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cinematographer, editor, composer, producers, production designer, art director, costume designer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function scrapes the names of different types of artists. The type must be specified in the 'artist' argument. \n",
    "\n",
    "In the \"Full Cast & Crew\" pages each artist type has its own table element containing the names of the artists. Each of these tables is preceded by an h4 element with an id equal to the artist type. For example, the name of the cinematographer is contained in the table preceded by the h4 element with id equal to \"cinematographer\". To scrape this data, we'll first access the h4 element, then we'll access the table next to it and finally we'll get all the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_artist(soup, artist):\n",
    "    \n",
    "    try:\n",
    "        h = soup.find(\"h4\", id = artist)\n",
    "        \n",
    "        a_tags = h.find_next(\"table\").find_all(\"a\")\n",
    "        \n",
    "        artists = [a.text.lstrip().replace('\\n', '') for a in a_tags]\n",
    "        \n",
    "        artists = ';'.join(artists)\n",
    "    \n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "    else:     \n",
    "        return artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we'll split the list of urls into 5 equally-sized chunks and wait 5 mins between each chunk so as not to clutter the IMDB website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scraped_df_2(urls):\n",
    "    \n",
    "    df = pd.DataFrame(columns = [\"id\", \"actors\", \"cinematographer\", \"editor\", \"composer\", \"producers\", \"production_designer\",\n",
    "                                \"art_director\", \"costume_designer\"])\n",
    "    \n",
    "    chunks = np.array_split(urls, 5)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        for url in chunk:\n",
    "            content = requests.get(url).content\n",
    "            soup = BeautifulSoup(content)\n",
    "            \n",
    "            df = df.append({\"id\": re.findall(r\"tt\\d+\", url)[0],\n",
    "                           \"actors\": scrape_actors(soup),\n",
    "                           \"cinematographer\": scrape_artist(soup, \"cinematographer\"),\n",
    "                           \"editor\": scrape_artist(soup, \"editor\"),\n",
    "                           \"composer\": scrape_artist(soup, \"composer\"),\n",
    "                           \"producers\": scrape_artist(soup, \"producer\"),\n",
    "                           \"production_designer\": scrape_artist(soup, \"production_designer\"),\n",
    "                           \"art_director\": scrape_artist(soup, \"art_director\"),\n",
    "                           \"costume_designer\": scrape_artist(soup, \"costume_designer\")},\n",
    "                           ignore_index = True)\n",
    "        \n",
    "        time.sleep(60*5)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it on our list of 'Full cast and crew' pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = build_scraped_df_2(crew_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHuYXHjmFdnU"
   },
   "source": [
    "# Load the data onto a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeXDWGQvFiU2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Building a database of IMDB 10,000 most popular feature films.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
