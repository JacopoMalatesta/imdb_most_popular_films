{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- salvare df_api\n",
    "- capire come scrapare meglio user review counts\n",
    "- join df, df2, (df3)\n",
    "- pulizia dati (contare nulle, casting)\n",
    "- 3 valori nulli in dataframe 1\n",
    "- script con funzioni???\n",
    "\n",
    "- aggiungere table of contents\n",
    "- commentare tutto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGNNMM5OucPo"
   },
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Tq5zlLcDuCkM"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GySk7pbunwY"
   },
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-6kyKK3UprX"
   },
   "source": [
    "IMDB doesn't have an official API. However, there are two unofficial APIs providing access to IMDB data: The Open Movie Database (OMDb) and The Movie Database (TMDB). In this project we'll rely on the latter as it doesn't enforce a rate limit. We're also going to supplement the API data with scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt0ZSUT3Xm3o"
   },
   "source": [
    "The TMDB API works by supplying the ID of the film(s) whose info we'd like to retrieve. The IDs consist of a double \"t\" followed by seven or eight digits. They can be found inside a film URL. For example, \"tt0050782\" is the ID for this film: https://www.imdb.com/title/tt0050782/. \n",
    "\n",
    "The first step will thus be retrieving the IDs of the 10 000 most popular films on IMDB, which can be found in this [playlist](https://www.imdb.com/search/keyword/?mode=detail&page=1&title_type=movie&ref_=kw_ref_rt_vt&num_votes=5000%2C&sort=num_votes,desc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruIEsF-V4A7N"
   },
   "source": [
    "## Scraping the film IDs and the URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MN3DXnxMYrbc"
   },
   "source": [
    "To scrape the film IDs of our 10 000 films we'll need to iterate a for loop over the first 200 pages in the playlist (each containing 50 films). The film ID is the value of the 'href' attribute inside h3 elements with the \"lister-item-header\" class. We'll write a simple regex to extract the film ID. We'll also scrape the entire URL: it's going to come in handy later when we do the scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N0QXTJHZwTUc"
   },
   "outputs": [],
   "source": [
    "film_ids = []\n",
    "film_urls = []\n",
    "\n",
    "for i in range(1, 201):\n",
    "  \n",
    "  content = requests.get(f\"https://www.imdb.com/search/keyword/?mode=detail&page={i}&title_type=movie&ref_=kw_ref_rt_vt&num_votes=5000%2C&sort=num_votes,desc\").content\n",
    "  \n",
    "  soup = BeautifulSoup(content)\n",
    "\n",
    "  for film in soup.find_all(\"h3\", class_=\"lister-item-header\"):\n",
    "\n",
    "    link = film.find(\"a\").get(\"href\")\n",
    "    film_id = re.findall(pattern = r\"tt\\d+\", string = link)[0]\n",
    "    film_ids.append(film_id)\n",
    "\n",
    "    film_url = \"https://www.imdb.com\" + link\n",
    "    film_urls.append(film_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GwT4Wuv61we"
   },
   "source": [
    "## API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51KySz-jZsxT"
   },
   "source": [
    "We'll now write a function to request the data from TMDB. The function takes a vector of film IDs as input and it returns a dataframe. For each ID it sends a GET request to the API. If the status code is equal to 200 (i.e. the request has been successful) it appends the data to a Pandas dataframe. Otherwise, it just appends the film_id and fills the remaining columns with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Pn5hoWsWdNZZ"
   },
   "outputs": [],
   "source": [
    "def build_film_df(film_ids):\n",
    "    \n",
    "  api_key = os.environ.get(\"tmdb_api_key\")\n",
    "\n",
    "  df = pd.DataFrame(columns = [\"id\", \"title\", \"release_date\", \"runtime\", \"country\", \"language\", \n",
    "                               \"genre\", \"studios\", \"budget\", \"revenue\"])\n",
    "\n",
    "  for film_id in film_ids:\n",
    "      \n",
    "      response = requests.get(f\"https://api.themoviedb.org/3/movie/{film_id}?api_key={api_key}\")\n",
    "\n",
    "      if response.status_code == 200:  \n",
    "\n",
    "        response_json = response.json()\n",
    "\n",
    "        df = df.append({\"id\": response_json[\"imdb_id\"],\n",
    "                      \"title\": response_json[\"title\"],\n",
    "                      \"release_date\": response_json[\"release_date\"],\n",
    "                      \"runtime\": response_json[\"runtime\"],\n",
    "                      \"country\": ';'.join([country['name'] for country in response_json[\"production_countries\"]]),\n",
    "                      \"language\": ';'.join([language[\"english_name\"] for language in response_json[\"spoken_languages\"]]),\n",
    "                      \"genre\": ';'.join([genre[\"name\"] for genre in response_json[\"genres\"]]),\n",
    "                      \"studios\": ';'.join([company[\"name\"] for company in response_json['production_companies']]),\n",
    "                      \"budget\": response_json['budget'],\n",
    "                      \"revenue\": response_json[\"revenue\"]}, \n",
    "                       ignore_index = True)\n",
    "        \n",
    "      else:\n",
    "        df = df.append({\"id\": film_id}, ignore_index = True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it on the film IDs we just scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "ckKclT-ouHfd",
    "outputId": "d438579e-4d83-4c41-aad8-e53f0c02135b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>country</th>\n",
       "      <th>language</th>\n",
       "      <th>genre</th>\n",
       "      <th>studios</th>\n",
       "      <th>budget</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0111161</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994-09-23</td>\n",
       "      <td>142</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>English</td>\n",
       "      <td>Drama;Crime</td>\n",
       "      <td>Castle Rock Entertainment</td>\n",
       "      <td>25000000</td>\n",
       "      <td>28341469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0468569</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008-07-14</td>\n",
       "      <td>152</td>\n",
       "      <td>United Kingdom;United States of America</td>\n",
       "      <td>English;Mandarin</td>\n",
       "      <td>Drama;Action;Crime;Thriller</td>\n",
       "      <td>DC Comics;Legendary Pictures;Syncopy;Isobel Gr...</td>\n",
       "      <td>185000000</td>\n",
       "      <td>1004558444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt1375666</td>\n",
       "      <td>Inception</td>\n",
       "      <td>2010-07-15</td>\n",
       "      <td>148</td>\n",
       "      <td>United Kingdom;United States of America</td>\n",
       "      <td>English;Japanese</td>\n",
       "      <td>Action;Science Fiction;Adventure</td>\n",
       "      <td>Legendary Pictures;Syncopy;Warner Bros. Pictures</td>\n",
       "      <td>160000000</td>\n",
       "      <td>825532764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0137523</td>\n",
       "      <td>Fight Club</td>\n",
       "      <td>1999-10-15</td>\n",
       "      <td>139</td>\n",
       "      <td>Germany;United States of America</td>\n",
       "      <td>English</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Regency Enterprises;Fox 2000 Pictures;Taurus F...</td>\n",
       "      <td>63000000</td>\n",
       "      <td>100853753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0109830</td>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>1994-07-06</td>\n",
       "      <td>142</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>English</td>\n",
       "      <td>Comedy;Drama;Romance</td>\n",
       "      <td>Paramount;The Steve Tisch Company</td>\n",
       "      <td>55000000</td>\n",
       "      <td>677387716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                     title release_date runtime  \\\n",
       "0  tt0111161  The Shawshank Redemption   1994-09-23     142   \n",
       "1  tt0468569           The Dark Knight   2008-07-14     152   \n",
       "2  tt1375666                 Inception   2010-07-15     148   \n",
       "3  tt0137523                Fight Club   1999-10-15     139   \n",
       "4  tt0109830              Forrest Gump   1994-07-06     142   \n",
       "\n",
       "                                   country          language  \\\n",
       "0                 United States of America           English   \n",
       "1  United Kingdom;United States of America  English;Mandarin   \n",
       "2  United Kingdom;United States of America  English;Japanese   \n",
       "3         Germany;United States of America           English   \n",
       "4                 United States of America           English   \n",
       "\n",
       "                              genre  \\\n",
       "0                       Drama;Crime   \n",
       "1       Drama;Action;Crime;Thriller   \n",
       "2  Action;Science Fiction;Adventure   \n",
       "3                             Drama   \n",
       "4              Comedy;Drama;Romance   \n",
       "\n",
       "                                             studios     budget     revenue  \n",
       "0                          Castle Rock Entertainment   25000000    28341469  \n",
       "1  DC Comics;Legendary Pictures;Syncopy;Isobel Gr...  185000000  1004558444  \n",
       "2   Legendary Pictures;Syncopy;Warner Bros. Pictures  160000000   825532764  \n",
       "3  Regency Enterprises;Fox 2000 Pictures;Taurus F...   63000000   100853753  \n",
       "4                  Paramount;The Steve Tisch Company   55000000   677387716  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = build_film_df(film_ids)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZmSWuoY8zvF"
   },
   "source": [
    "Let's have a look at the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "9Z4dClmq8WuR",
    "outputId": "3efdddf4-1f69-4614-b5c4-2a02803f79f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>country</th>\n",
       "      <th>language</th>\n",
       "      <th>genre</th>\n",
       "      <th>studios</th>\n",
       "      <th>budget</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5240</th>\n",
       "      <td>tt9573980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9131</th>\n",
       "      <td>tt8075016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>tt13423846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id title release_date runtime country language genre studios  \\\n",
       "5240   tt9573980   NaN          NaN     NaN     NaN      NaN   NaN     NaN   \n",
       "9131   tt8075016   NaN          NaN     NaN     NaN      NaN   NaN     NaN   \n",
       "9982  tt13423846   NaN          NaN     NaN     NaN      NaN   NaN     NaN   \n",
       "\n",
       "     budget revenue  \n",
       "5240    NaN     NaN  \n",
       "9131    NaN     NaN  \n",
       "9982    NaN     NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save it as a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_api.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VroIT9ExpVtY"
   },
   "source": [
    "## Scraping additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOgUWYm7bRqV"
   },
   "source": [
    "TMDB API has some limits: it doesn't provide data about the people who worked on a film (directors, writer, actors etc.) and its rating data is inaccurate. We'll thus integrate the data we just got by scraping some more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdfF63LCTwRi"
   },
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KytHOgpT0HC"
   },
   "source": [
    "To implement sound software engineering principles we'll scrape the data by building a function for each type of data we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdS235LCUFDf"
   },
   "source": [
    "#### Film ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nNSWeewsQ3Gt"
   },
   "outputs": [],
   "source": [
    "def scrape_film_id(soup):\n",
    "    \n",
    "    try:\n",
    "        film_id = soup.find(\"meta\", {\"property\": \"imdb:pageConst\"}).get(\"content\")\n",
    "    except:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return film_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WpHvI39UHtL"
   },
   "source": [
    "#### Directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zwLDT6CW1tL0"
   },
   "outputs": [],
   "source": [
    "def scrape_director(soup):\n",
    "    \n",
    "    try:  \n",
    "        a_tags = soup.find_all(href = re.compile(\"tt_ov_dr\"), class_=\"ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link\")\n",
    "    \n",
    "        directors = list(set([a.text for a in a_tags]))\n",
    "    \n",
    "        directors = ';'.join(directors)\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "    else:\n",
    "        return directors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_writer(soup):\n",
    "    \n",
    "    try:\n",
    "        a_tags = soup.find_all(href = re.compile(\"tt_ov_wr\"), class_=\"ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link\")\n",
    "        \n",
    "        writers = list(set([a.text for a in a_tags]))\n",
    "        \n",
    "        writers = ';'.join(writers)\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "    else:\n",
    "        return writers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY0xRacoUKxG"
   },
   "source": [
    "#### IMDB average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GHuWP7Pe8EHq"
   },
   "outputs": [],
   "source": [
    "def scrape_imdb_rating(soup):\n",
    "    \n",
    "    try: \n",
    "        pattern = r'(?<=\"ratingValue\":)[\\d.]+(?=},\"contentRating\")'\n",
    "        string = str(soup.find(\"script\", {\"type\": \"application/ld+json\"}))\n",
    "        rating = re.findall(pattern = pattern, string = string)[0]\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return rating\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63sBX-TwUN1q"
   },
   "source": [
    "#### IMDB rating count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7u68JHWr_1oe"
   },
   "outputs": [],
   "source": [
    "def scrape_rating_count(soup):\n",
    "    \n",
    "    try:\n",
    "        pattern = r'(?<=\"ratingCount\":)[\\d.]+'\n",
    "        string = str(soup.find(\"script\", {\"type\": \"application/ld+json\"}))\n",
    "        rating_count = re.findall(pattern = pattern, string = string)[0]\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return rating_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne8ybLy_UPrk"
   },
   "source": [
    "#### Metascore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HzWpD51AAhe5"
   },
   "outputs": [],
   "source": [
    "def scrape_metascore(soup):\n",
    "    \n",
    "    try:\n",
    "        metascore = soup.find(\"span\", class_=\"score-meta\").text \n",
    "    \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return metascore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YwH147RUSXB"
   },
   "source": [
    "#### User review count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PLb_JqIRnKls"
   },
   "outputs": [],
   "source": [
    "def scrape_user_review_count(soup):\n",
    "    \n",
    "    try: \n",
    "        pattern = r'(?<=\"total\":)\\d+(?=,\"__typename\":\"ReviewsConnection\"},\"criticReviewsTotal\":)'\n",
    "  \n",
    "        string = str(soup.find(\"script\", {'id': '__NEXT_DATA__'}))\n",
    "    \n",
    "        user_review_count = re.findall(pattern = pattern, string = string)[0]\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return user_review_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0WXIPIWUWHl"
   },
   "source": [
    "#### Critic review count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_critic_review_count(soup):\n",
    "    \n",
    "    try:   \n",
    "        spans = soup.find_all(\"span\", class_=\"three-Elements\") \n",
    "        \n",
    "        string = list(filter(lambda x: 'Critic' in str(x), spans))[0].text\n",
    "        \n",
    "        critic_review_count = re.findall(r'\\d+', string)[0]\n",
    "    \n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "    else:\n",
    "        return critic_review_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_color(soup):\n",
    "    \n",
    "    try:\n",
    "        pattern = r'(?<=\"text\":\")\\w+(?=\",\"attributes\":\\[\\],\"__typename\":\"Coloration\"})'\n",
    "        \n",
    "        string = str(soup.find(\"script\", {\"type\": \"application/json\"}))\n",
    "    \n",
    "        color = re.findall(pattern = pattern, string = string)[0]\n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_aspect_ratio(soup):\n",
    "    try:\n",
    "        pattern = r'(?<=\"aspectRatio\":\")[\\d.\\s:]+'\n",
    "    \n",
    "        string = str(soup.find(\"script\", {\"type\": \"application/json\"}))\n",
    "    \n",
    "        aspect_ratio = re.findall(pattern = pattern, string = string)[0] \n",
    "        \n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "    else:\n",
    "        return aspect_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRaqDP82V4Xh"
   },
   "source": [
    "### Creating the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we send 10 000 requests at once we're likely going to run into problems with IMDB, so let's split the film_urls list into 5 equally-sized chunks and let's scrape each chunk at a time by waiting five minutes before moving to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PuxtrS5V8IQ"
   },
   "outputs": [],
   "source": [
    "def build_scraped_df(urls):\n",
    "\n",
    "  df = pd.DataFrame(columns = [\"id\", \"director\", \"writer\", \"imdb_rating\", \"imdb_rating_count\", \"metascore\", \n",
    "                               \"user_review_count\", \"critic_review_count\", \"color\", \"aspect_ratio\"])\n",
    "\n",
    "  chunks = np.array_split(urls, 5)\n",
    "\n",
    "  for chunk in chunks:\n",
    "    for url in chunk:\n",
    "      content = requests.get(url).content\n",
    "      soup = BeautifulSoup(content)\n",
    "\n",
    "      df = df.append({\"id\": scrape_film_id(soup),\n",
    "                      \"director\": scrape_director(soup),\n",
    "                      \"writer\": scrape_writer(soup),\n",
    "                      \"imdb_rating\": scrape_imdb_rating(soup),\n",
    "                      \"imdb_rating_count\": scrape_rating_count(soup),\n",
    "                      \"metascore\": scrape_metascore(soup),\n",
    "                      \"user_review_count\": scrape_user_review_count(soup),\n",
    "                      \"critic_review_count\": scrape_critic_review_count(soup),\n",
    "                      \"color\": scrape_color(soup),\n",
    "                      \"aspect_ratio\": scrape_aspect_ratio(soup)}, \n",
    "                      ignore_index = True)\n",
    "\n",
    "    time.sleep(60*5)\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PuxtrS5V8IQ"
   },
   "outputs": [],
   "source": [
    "def build_scraped_df(urls):\n",
    "\n",
    "  df = pd.DataFrame(columns = [\"id\", \"director\", \"writer\", \"imdb_rating\", \"imdb_rating_count\", \"metascore\", \n",
    "                               \"user_review_count\", \"critic_review_count\", \"color\", \"aspect_ratio\"])\n",
    "\n",
    "  for url in urls:\n",
    "        \n",
    "        session = requests.Session()\n",
    "        retry = Retry(total = 10, backoff_factor = 1)\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "        content = session.get(url).content\n",
    "        soup = BeautifulSoup(content)\n",
    "        \n",
    "        df = df.append({\"id\": scrape_film_id(soup),\n",
    "                        \"director\": scrape_director(soup),\n",
    "                        \"writer\": scrape_writer(soup),\n",
    "                        \"imdb_rating\": scrape_imdb_rating(soup),\n",
    "                        \"imdb_rating_count\": scrape_rating_count(soup),\n",
    "                        \"metascore\": scrape_metascore(soup),\n",
    "                        \"user_review_count\": scrape_user_review_count(soup),\n",
    "                        \"critic_review_count\": scrape_critic_review_count(soup),\n",
    "                        \"color\": scrape_color(soup),\n",
    "                        \"aspect_ratio\": scrape_aspect_ratio(soup)}, \n",
    "                        ignore_index = True)\n",
    "\n",
    "\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "Sqmef_HbZ6fi",
    "outputId": "a5e98cbb-4ae7-4444-9e39-bfe70225c3d4"
   },
   "outputs": [],
   "source": [
    "df2 = build_scraped_df(film_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "Sqmef_HbZ6fi",
    "outputId": "a5e98cbb-4ae7-4444-9e39-bfe70225c3d4"
   },
   "outputs": [],
   "source": [
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the null values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is each id unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping people data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, TMDB API doesn't provide much data about the people who work on a film, so we'll need to scrape that data ourselves. These pieces of info aren't located in the regular film pages but in the \"Full cast and crew\" pages. So before scraping the data we first need to collect all these pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_pages = []\n",
    "\n",
    "for url in film_urls:\n",
    "        \n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    a_target = soup.find(href = re.compile(\"tt_cl_sm\"), class_=\"ipc-metadata-list-item__label ipc-metadata-list-item__label--link\")\n",
    "    \n",
    "    crew_page = \"https://www.imdb.com\" + a_target.get(\"href\")\n",
    "    \n",
    "    crew_pages.append(crew_page)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQtgCMgbUgCS"
   },
   "source": [
    "#### Full cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the artists can be found inside the table with the \"cast-list\" class: they're the alternative text of the images inside the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_actors(soup):\n",
    "    \n",
    "    try:   \n",
    "        images = soup.find(\"table\", class_=\"cast_list\").find_all(\"img\") \n",
    "        \n",
    "        actors = [img.get(\"alt\") for img in images]  \n",
    "        \n",
    "        actors = ';'.join(actors)\n",
    "    \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        return actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cinematographer, editor, composer, producers, production designer, art director, costume designer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function scrapes the names of different types of artists. The type must be specified in the 'artist' argument. \n",
    "\n",
    "In the \"Full Cast & Crew\" pages each artist type has its own table element containing the names of the artists. Each of these tables is preceded by an h4 element with an id equal to the artist type. For example, the name of the cinematographer is contained in the table preceded by the h4 element with id equal to \"cinematographer\". To scrape this data, we'll first access the h4 element, then we'll access the table next to it and finally we'll get all the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_artist(soup, artist):\n",
    "    \n",
    "    try:\n",
    "        h = soup.find(\"h4\", id = artist)\n",
    "        \n",
    "        a_tags = h.find_next(\"table\").find_all(\"a\")\n",
    "        \n",
    "        artists = [a.text.lstrip().replace('\\n', '') for a in a_tags]\n",
    "        \n",
    "        artists = ';'.join(artists)\n",
    "    \n",
    "    except:\n",
    "        return np.nan\n",
    "        \n",
    "    else:     \n",
    "        return artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we'll split the list of urls into 5 equally-sized chunks and wait 5 mins between each chunk so as not to clutter the IMDB website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scraped_df_2(urls):\n",
    "    \n",
    "    df = pd.DataFrame(columns = [\"id\", \"actors\", \"cinematographer\", \"editor\", \"composer\", \"producers\", \"production_designer\",\n",
    "                                \"art_director\", \"costume_designer\"])\n",
    "    \n",
    "    chunks = np.array_split(urls, 5)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        for url in chunk:\n",
    "            content = requests.get(url).content\n",
    "            soup = BeautifulSoup(content)\n",
    "            \n",
    "            df = df.append({\"id\": re.findall(r\"tt\\d+\", url)[0],\n",
    "                           \"actors\": scrape_actors(soup),\n",
    "                           \"cinematographer\": scrape_artist(soup, \"cinematographer\"),\n",
    "                           \"editor\": scrape_artist(soup, \"editor\"),\n",
    "                           \"composer\": scrape_artist(soup, \"composer\"),\n",
    "                           \"producers\": scrape_artist(soup, \"producer\"),\n",
    "                           \"production_designer\": scrape_artist(soup, \"production_designer\"),\n",
    "                           \"art_director\": scrape_artist(soup, \"art_director\"),\n",
    "                           \"costume_designer\": scrape_artist(soup, \"costume_designer\")},\n",
    "                           ignore_index = True)\n",
    "        \n",
    "        time.sleep(60*5)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it on our list of 'Full cast and crew' pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = build_scraped_df_2(crew_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHuYXHjmFdnU"
   },
   "source": [
    "# Load the data onto a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeXDWGQvFiU2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Building a database of IMDB 10,000 most popular feature films.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
